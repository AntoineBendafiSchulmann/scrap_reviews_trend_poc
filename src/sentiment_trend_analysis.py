import os
import re
import torch
import spacy
import pandas as pd
from collections import Counter
from keybert import KeyBERT
from yake import KeywordExtractor
from transformers import AutoModelForCausalLM, AutoTokenizer

nlp = spacy.load("fr_core_news_md")

model_name = "bigscience/bloomz-7b1-mt"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto" if torch.cuda.is_available() else "cpu",torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

TREND_INPUT_FILE = os.path.join(BASE_DIR, "trustpilot_reviews_with_sentiment_camembert.txt")
TREND_OUTPUT_FILE = os.path.join(BASE_DIR, "trustpilot_sentiment_trends.txt")

kw_extractor = KeywordExtractor(lan="fr", n=5, top=50)
kw_model = KeyBERT("all-mpnet-base-v2")

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^\w\s']", "", text)
    return text.strip()

def extract_trends(texts, sentiment, top_n=20):
    if not texts:
        return ["Aucune tendance dÃ©tectÃ©e"]

    all_trends = []
    for text in texts:
        yake_keywords = kw_extractor.extract_keywords(text)
        keybert_keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(3, 6), stop_words="french", top_n=5)

        all_trends.extend([kw[0] for kw in yake_keywords if len(kw[0].split()) > 4])
        all_trends.extend([kw[0] for kw in keybert_keywords if len(kw[0].split()) > 4])

    word_freq = Counter(all_trends)
    extracted_trends = list(set([
        phrase for phrase, _ in word_freq.most_common(top_n)
        if phrase not in ["bonjour", "rien dire", "merci"]
    ]))

    refined_trends = refine_trends(extracted_trends)
    
    filtered_trends = [
        trend for trend in refined_trends
        if len(trend.split()) > 3 and not any(
            word in trend.lower() for word in ["oreille", "bonjour", "rien dire", "merci"]
        )
    ]

    print(f"âœ… Tendances filtrÃ©es ({sentiment}) :", filtered_trends if filtered_trends else "Aucune tendance dÃ©tectÃ©e")
    return filtered_trends if filtered_trends else ["Aucune tendance dÃ©tectÃ©e"]

def refine_trends(trends):
    if not trends or trends == ["Aucune tendance dÃ©tectÃ©e"]:
        return ["Aucune tendance dÃ©tectÃ©e"]

    refined_trends = []
    seen_trends = set()

    for trend in trends:
        trend = trend.replace("rge vendre de faux panneaux", "arnaque sur les panneaux solaires")
        trend = trend.replace("cofidis et sociÃ©tÃ© planÃ¨te Ã©cologique", "cofidis et une entreprise Ã©cologique")
        trend = trend.replace("montant mais globalement la conseillÃ¨re", "avis sur le montant et le conseil")

        doc = nlp(trend)
        if len(doc) > 2 and not any(nlp(existing).similarity(doc) > 0.85 for existing in seen_trends):
            seen_trends.add(trend)
            refined_trends.append(trend)

    print(f"âœ… Tendances affinÃ©es : {len(refined_trends)} tendances significatives.")
    return refined_trends

def generate_summary(trends, sentiment_type, retry=0):
    if not trends or trends == ["Aucune tendance dÃ©tectÃ©e"]:
        return "Aucune idÃ©e gÃ©nÃ©rale dÃ©tectÃ©e."

    try:
        filtered_trends = [t for t in trends if len(t.split()) > 2]
        if not filtered_trends:
            print(f"âš ï¸ Aucune tendance exploitable trouvÃ©e pour {sentiment_type}.")
            return "RÃ©sumÃ© non disponible."

        trends_text = "; ".join(filtered_trends) + "."
        prompt = f"""Les avis {sentiment_type} mentionnent principalement ces tendances : {trends_text} 
        ðŸ’¡ Reformule cela en UNE SEULE PHRASE naturelle et fluide, sans liste brute, en reliant logiquement les idÃ©es.
        ðŸ”¹ Ne change pas le sens des tendances.
        ðŸ”¹ Ne gÃ©nÃ¨re pas plus dâ€™une phrase.
        """

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        output_ids = model.generate(
            **inputs,
            max_new_tokens=80,  
            pad_token_id=tokenizer.eos_token_id,  
            temperature=0.7, 
            top_p=0.9,  
            repetition_penalty=1.1,  
            do_sample=True  
        )

        response = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

        if response.lower() in ["rÃ©sumÃ© non disponible", "aucune idÃ©e gÃ©nÃ©rale dÃ©tectÃ©e"]:
            return response

        if response.count(",") > 10 or ":" in response:
            if retry < 2:
                print(f"âš ï¸ RÃ©sumÃ© encore trop hachÃ©, reformulation pour {sentiment_type}. Tentative {retry+1}.")
                return generate_summary(filtered_trends[:10], sentiment_type, retry=retry+1)
            else:
                return "RÃ©sumÃ© non disponible."

        return response

    except Exception as e:
        print(f"âŒ Erreur dans `generate_summary({sentiment_type})` : {e}")
        return "RÃ©sumÃ© non gÃ©nÃ©rÃ©."

def format_summary(trends):
    if not trends or trends == ["Aucune tendance dÃ©tectÃ©e"]:
        return "Aucune tendance dÃ©tectÃ©e."
    return "\n".join(f"- {trend}" for trend in trends)

def main():
    print("ðŸ› ï¸ Test rapide du modÃ¨le...")  # si le rÃ©sultat est trop bizarre, essayer de changer le modÃ¨le ou le prompt
    test_trends = ["rÃ©ponse rapide", "dossier traitÃ© rapidement", "satisfait du service"]
    test_summary = generate_summary(test_trends, "positifs")
    print(f"âœ… Test du modÃ¨le : {test_summary}") 

    print(" Lecture du fichier de tendances...")
    df = pd.read_csv(TREND_INPUT_FILE, sep="\t", header=None, names=["text", "sentiment"])
    print("âœ… Fichier chargÃ© avec succÃ¨s.")

    df["clean_text"] = df["text"].apply(clean_text)
    print(" Nettoyage des avis terminÃ©.")

    pos_reviews = df[df["sentiment"] == "POSITIVE"]["clean_text"].tolist()
    neg_reviews = df[df["sentiment"] == "NEGATIVE"]["clean_text"].tolist()
    neu_reviews = df[df["sentiment"] == "NEUTRAL"]["clean_text"].tolist()

    print(" DÃ©but de l'extraction des tendances...")
    pos_trends = extract_trends(pos_reviews, "positif")
    neg_trends = extract_trends(neg_reviews, "nÃ©gatif")
    neu_trends = extract_trends(neu_reviews, "neutre")

    print(" Affinage des tendances...")
    pos_trends = refine_trends(pos_trends)
    neg_trends = refine_trends(neg_trends)
    neu_trends = refine_trends(neu_trends)

    print(f"ðŸ” VÃ©rification - Tendances positives envoyÃ©es au rÃ©sumÃ© : {pos_trends}")
    print(f"ðŸ” VÃ©rification - Tendances nÃ©gatives envoyÃ©es au rÃ©sumÃ© : {neg_trends}")
    print(f"ðŸ” VÃ©rification - Tendances neutres envoyÃ©es au rÃ©sumÃ© : {neu_trends}")

    print(" GÃ©nÃ©ration des rÃ©sumÃ©s avec le LLM local...")
    pos_summary = generate_summary(pos_trends, "positifs")
    neg_summary = generate_summary(neg_trends, "nÃ©gatifs")
    neu_summary = generate_summary(neu_trends, "neutres")

    print("Enregistrement des rÃ©sultats dans le fichier de sortie...")
    with open(TREND_OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("RÃ©partition des sentiments :\n")
        f.write(f"Positifs : {len(pos_reviews)} avis ({len(pos_reviews) / len(df) * 100:.2f}%)\n")
        f.write(f"NÃ©gatifs : {len(neg_reviews)} avis ({len(neg_reviews) / len(df) * 100:.2f}%)\n")
        f.write(f"Neutres  : {len(neu_reviews)} avis ({len(neu_reviews) / len(df) * 100:.2f}%)\n\n")

        f.write("**SynthÃ¨se des avis positifs :**\n")
        f.write(pos_summary + "\n\n")
        f.write("**Tendances extraites (positif) :**\n")
        f.write(format_summary(pos_trends) + "\n\n")

        f.write("**SynthÃ¨se des avis nÃ©gatifs :**\n")
        f.write(neg_summary + "\n\n")
        f.write("**Tendances extraites (nÃ©gatif) :**\n")
        f.write(format_summary(neg_trends) + "\n\n")

        f.write("**SynthÃ¨se des avis neutres :**\n")
        f.write(neu_summary + "\n\n")
        f.write("**Tendances extraites (neutre) :**\n")
        f.write(format_summary(neu_trends) + "\n\n")

    print(f"âœ… RÃ©sumÃ© enregistrÃ© dans `{TREND_OUTPUT_FILE}`.")

if __name__ == "__main__":
    main()